{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: **Data Jobs Salaries in Mexico in January 2022**\n",
    "\n",
    "## 1. Data Collection: Web scraping\n",
    "\n",
    "##### Author: **Daniel Eduardo LÃ³pez (DanielEduardoLopez)**\n",
    "##### GitHub: **_https://github.com/DanielEduardoLopez_**\n",
    "##### Contact: **_daniel-eduardo-lopez@outlook.com_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importation\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape job data from OCC.com.mx\n",
    "def occscraper(jobs_list, pages, vacancy_class, jobname_class, salary_class, company_class, location_class):\n",
    "\"\"\"\n",
    "This function scrapes job data from the OCC Website (occ.com.mx): Position Name, Salary, Company and Location.\n",
    "\n",
    "It requires 7 inputs: \n",
    "1. jobs_list : List with the name of the Data Jobs in both English and Spanish and avoiding empty words (Python list of strings).\n",
    "2. pages : Number of pages to scrap from the website (Integer).\n",
    "3. vacancy_class : Class identifier for the vacancy, for instance: 'c0132 c011010' (String)\n",
    "4. jobname_class : Class identifier for the name of the position, for instance: 'c01584 c01588 c01604 c01990 c011016' (String)\n",
    "5. salary_class : Class identifier for the salary of the position, for instance: 'c01584 c01591 c01604 c01993' (String)\n",
    "6. company_class : Class identifier for the company offering the position, for instance: 'c011000' (String)\n",
    "7. location_class : Class identifier for the geographical location of the position, for instance: 'c011005 c011006' (String)\n",
    "\n",
    "IMPORTANT NOTE: OCC Website dynamically sets the class identifiers for its page elements. So, surely the example class \n",
    "identifiers will not produce results when running the present code in a different moment than the one when this code was \n",
    "written and run. Thus, to RE-RUN the code, first, it is necessary to the load the page source into the variable html_test \n",
    "as shown below:\n",
    "\n",
    "driver.get(jobs_url_list[0])\n",
    "html_test = driver.page_source\n",
    "html_test\n",
    "\n",
    "And then INSPECT what are the CURRENT class identifiers to produce NEW results.\n",
    "\n",
    "Output: Pandas Dataframe with the results from the web scraping.\n",
    "\"\"\"\n",
    "\n",
    "    # Setting of the base url of the OCC searcher\n",
    "    base_url = \"https://www.occ.com.mx/empleos/de-\"\n",
    "    base_page_url = \"?page=\"\n",
    "\n",
    "    # Creation of the corresponding url for each job from the jobs list\n",
    "    jobs_url_list = list(jobs_list)\n",
    "    length = len(jobs_url_list)\n",
    "\n",
    "    for i in range(0,length):\n",
    "        jobs_url_list[i] = jobs_url_list[i].strip()\n",
    "        jobs_url_list[i] = jobs_url_list[i].lower()\n",
    "        jobs_url_list[i] = jobs_url_list[i].replace(' ','-')\n",
    "        jobs_url_list[i] = base_url + jobs_url_list[i]\n",
    "        jobs_url_list[i] = jobs_url_list[i] + '/'\n",
    "        #print(jobs_url_list[i])\n",
    "\n",
    "    # Setting of the executable path in a new service instance for \n",
    "    service = Service(executable_path=GeckoDriverManager().install())\n",
    "\n",
    "    # Creation of a new instance of the Firefox driver\n",
    "    driver = webdriver.Firefox(service = service)\n",
    "\n",
    "    # Creation of the list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Iterations over the different jobs\n",
    "    for job_url in jobs_url_list:\n",
    "        \n",
    "        # Start of the loop\n",
    "        print('Fetching data for:', jobs_list[jobs_url_list.index(job_url)].title(), \n",
    "            ' ({} out of {})'.format(jobs_url_list.index(job_url)+1, length))\n",
    "        \n",
    "        # Creation of the different pages for the job\n",
    "        pages_url_list = []\n",
    "        for j in range(1, number_pages + 1):\n",
    "            if j == 1:\n",
    "                pages_url_list.append(job_url)\n",
    "            else:\n",
    "                pages_url_list.append(job_url + base_page_url + str(j))\n",
    "            \n",
    "        # Web scrapping over the different pages\n",
    "        for url in pages_url_list:\n",
    "            \n",
    "            # Adding try tag in case urls might have a problem\n",
    "            try:\n",
    "                # Soup creation\n",
    "                driver.get(url)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                \n",
    "                # Data extraction\n",
    "                vacancies = soup.find_all('div', attrs = {'class': vacancy_class})\n",
    "                \n",
    "                for vacancy in vacancies:\n",
    "                    job = []\n",
    "                    \n",
    "                    try:\n",
    "                        job.append(vacancy.find('h2', attrs = {'class': jobname_class}).text)\n",
    "                    except:\n",
    "                        job.append(None) # In case there is no job name available\n",
    "                    \n",
    "                    try:\n",
    "                        job.append(vacancy.find('span', attrs = {'class': salary_class}).text)\n",
    "                    except:\n",
    "                        job.append(None) # In case there is no salary available\n",
    "                    \n",
    "                    try:\n",
    "                        job.append(vacancy.find('a', attrs = {'class': company_class}).text)\n",
    "                    except:\n",
    "                        job.append(None) # In case there is no company name available\n",
    "\n",
    "                    try:\n",
    "                        job.append(vacancy.find('a', attrs = {'class': location_class}).text)\n",
    "                    except:\n",
    "                        job.append(None) # In case there is no location available\n",
    "\n",
    "                    data.append(job)\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # End of the urls loop\n",
    "        print('Successfully retrieved data for:', jobs_list[jobs_url_list.index(job_url)].title(),\n",
    "            ' ({} out of {})'.format(jobs_url_list.index(job_url) + 1, length) +'\\n')\n",
    "\n",
    "    # End of the main loop\n",
    "    print('Job done!\\n\\n')\n",
    "\n",
    "    # Closure of the Driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Data storation as a data frame\n",
    "    df = pd.DataFrame(data, columns = ['Job','Salary','Company','Location'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry of the Data Jobs in both English and Spanish (avoid empty words) in a Python list\n",
    "\n",
    "jobs_list = [\"analista datos\",\n",
    "           \"data analyst\",\n",
    "           \"cientifico datos\",\n",
    "           \"data scientist\",\n",
    "           \"ingeniero datos\",\n",
    "           \"data engineer\",\n",
    "           \"arquitecto datos\",\n",
    "           \"data arquitect\",\n",
    "           \"analista negocio\",\n",
    "           \"business analyst\"]\n",
    "\n",
    "# This list was based on: \n",
    "    # Axistalent (2020). The Ecosystem of Data Jobs - Making sense of the Data Job Market. https://www.axistalent.io/blog/the-ecosystem-of-data-jobs-making-sense-of-the-data-job-market \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pages to scrap\n",
    "number_pages = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval of the class identifiers from the OCC Website\n",
    "\"\"\"\n",
    "IMPORTANT NOTE: OCC Website dynamically sets the class identifiers for its \n",
    "page elements. So, surely the following class identifiers will not produce \n",
    "results when running the present code in a different moment than the one\n",
    "when this code was written and run. \n",
    "Thus, to RE-RUN the code, first, it is necessary to the load the page source \n",
    "into the variable html_test as shown below and then INSPECT what are the \n",
    "CURRENT class identifiers to produce NEW results.\n",
    "\"\"\"\n",
    "\n",
    "driver.get(jobs_url_list[0])\n",
    "html_test = driver.page_source\n",
    "html_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry of the OCC Website class identifiers\n",
    "vacancy_class = 'c0132 c011010' # Done\n",
    "jobname_class = 'c01584 c01588 c01604 c01990 c011016' # Done\n",
    "salary_class = 'c01584 c01591 c01604 c01993' # Done\n",
    "company_class = 'c011000' # Done\n",
    "location_class = 'c011005 c011006' # Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exportation as a csv\n",
    "df.to_csv('DataJobsMexicoJan2022.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e975f9d2f60b4d9c1da07316fa345238690997a135e24c2e358becc5092ba7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
